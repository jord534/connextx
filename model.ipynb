{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jordanbonil/Documents/source/git/connextx/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "termcolor not installed, skipping dependency\n",
      "No pygame installed, ignoring import\n"
     ]
    }
   ],
   "source": [
    "from kaggle_environments import evaluate, make, utils\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "env = make(\"connectx\", debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Environment in module kaggle_environments.core object:\n",
      "\n",
      "class Environment(builtins.object)\n",
      " |  Environment(specification=None, configuration=None, info=None, steps=None, logs=None, agents=None, interpreter=None, renderer=None, html_renderer=None, debug=False, state=None)\n",
      " |\n",
      " |  Methods defined here:\n",
      " |\n",
      " |  __init__(self, specification=None, configuration=None, info=None, steps=None, logs=None, agents=None, interpreter=None, renderer=None, html_renderer=None, debug=False, state=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |\n",
      " |  clone(self)\n",
      " |      Returns:\n",
      " |          Environment: A copy of the current environment.\n",
      " |\n",
      " |  debug_print(self, message)\n",
      " |\n",
      " |  play(self, agents=None, **kwargs)\n",
      " |      Renders a visual representation of the environment and allows interactive action selection.\n",
      " |\n",
      " |      Args:\n",
      " |          **kwargs (dict): Args directly passed into render().  Mode is fixed to ipython.\n",
      " |\n",
      " |      Returns:\n",
      " |          None: prints directly to an IPython notebook\n",
      " |\n",
      " |  render(self, **kwargs)\n",
      " |      Renders a visual representation of the current state of the environment.\n",
      " |\n",
      " |      Args:\n",
      " |          mode (str): html, ipython, ansi, human (default)\n",
      " |          **kwargs (dict): Other args are directly passed into the html player.\n",
      " |\n",
      " |      Returns:\n",
      " |          str: html if mode=html or ansi if mode=ansi.\n",
      " |          None: prints ansi if mode=human or prints html if mode=ipython\n",
      " |\n",
      " |  reset(self, num_agents=None)\n",
      " |      Resets the environment state to the initial step.\n",
      " |\n",
      " |      Args:\n",
      " |          num_agents (int): Resets the state assuming a fixed number of agents.\n",
      " |\n",
      " |      Returns:\n",
      " |          list of dict: The agents states after the reset.\n",
      " |\n",
      " |  run(self, agents)\n",
      " |      Steps until the environment is \"done\" or the runTimeout was reached.\n",
      " |\n",
      " |      Args:\n",
      " |          agents (list of any): List of agents to obtain actions from.\n",
      " |\n",
      " |      Returns:\n",
      " |          tuple of:\n",
      " |              list of list of dict: The agent states of all steps executed.\n",
      " |              list of list of dict: The agent logs of all steps executed.\n",
      " |\n",
      " |  step(self, actions, logs=None)\n",
      " |      Execute the environment interpreter using the current state and a list of actions.\n",
      " |\n",
      " |      Args:\n",
      " |          actions (list): Actions to pair up with the current agent states.\n",
      " |          logs (list): Logs to pair up with each agent for the current step.\n",
      " |\n",
      " |      Returns:\n",
      " |          list of dict: The agents states after the step.\n",
      " |\n",
      " |  toJSON(self)\n",
      " |      Returns:\n",
      " |          dict: Specifcation and current state of the Environment instance.\n",
      " |\n",
      " |  train(self, agents=None)\n",
      " |      Setup a lightweight training environment for a single agent.\n",
      " |      Note: This is designed to be a lightweight starting point which can\n",
      " |            be integrated with other frameworks (i.e. gym, stable-baselines).\n",
      " |            The reward returned by the \"step\" function here is a diff between the\n",
      " |            current and the previous step.\n",
      " |\n",
      " |      Example:\n",
      " |          env = make(\"tictactoe\")\n",
      " |          # Training agent in first position (player 1) against the default random agent.\n",
      " |          trainer = env.train([None, \"random\"])\n",
      " |\n",
      " |          obs = trainer.reset()\n",
      " |          done = False\n",
      " |          while not done:\n",
      " |              action = 0 # Action for the agent being trained.\n",
      " |              obs, reward, done, info = trainer.step(action)\n",
      " |          env.render()\n",
      " |\n",
      " |      Args:\n",
      " |          agents (list): List of agents to obtain actions from while training.\n",
      " |                         The agent to train (in position), should be set to \"None\".\n",
      " |\n",
      " |      Returns:\n",
      " |          `dict`.reset: Reset def that reset the environment, then advances until the agents turn.\n",
      " |          `dict`.step: Steps using the agent action, then advance until agents turn again.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |\n",
      " |  done\n",
      " |      bool: If any agents have an ACTIVE status.\n",
      " |\n",
      " |  name\n",
      " |      str: The name from the specification.\n",
      " |\n",
      " |  version\n",
      " |      str: The version from the specification.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.state[1]['observation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'int' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m env\u001b[38;5;241m.\u001b[39mrender(mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mipython\u001b[39m\u001b[38;5;124m\"\u001b[39m, width\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m, height\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m450\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/source/git/connextx/.venv/lib/python3.12/site-packages/kaggle_environments/core.py:210\u001b[0m, in \u001b[0;36mEnvironment.step\u001b[0;34m(self, actions, logs)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone:\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m FailedPrecondition(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnvironment done, reset required.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m actions \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate):\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidArgument(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m actions required.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    213\u001b[0m action_state \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate)\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'int' has no len()"
     ]
    }
   ],
   "source": [
    "env.step(4)\n",
    "env.render(mode=\"ipython\", width=500, height=450)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This agent random chooses a non-empty column.\n",
    "def my_agent(observation, configuration):\n",
    "    from random import choice\n",
    "    return choice([c for c in range(configuration.columns) if observation.board[c] == 0])\n",
    "\n",
    "# Play as first position against random agent.\n",
    "trainer = env.train([None, \"random\"])\n",
    "\n",
    "observation = trainer.reset()\n",
    "\n",
    "while not env.done:\n",
    "    my_action = my_agent(observation, env.configuration)\n",
    "    print(\"My Action\", my_action)\n",
    "    observation, reward, done, info = trainer.step(my_action)\n",
    "    # env.render(mode=\"ipython\", width=100, height=90, header=False, controls=False)\n",
    "env.render(mode=\"ipython\", width=500, height=450)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.play([None, \"negamax\"], width=500, height=450)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build your model\n",
    "\n",
    "# It will be a Q learning model\n",
    "\n",
    "# it will have parameters:\n",
    "# - alpha (learning rate)\n",
    "# - Q(s,.) initialised for every state, so choose the quality of the state. except for terminal states, where Q(terminal = 0)\n",
    "# - some gamma (importance of future rewards) (discount factor)\n",
    "# - a reward function\n",
    "\n",
    "def preprocess_state(board, mark):\n",
    "    board_array = np.array(board).flatten()\n",
    "    state = np.concatenate([board_array, [mark]])\n",
    "    return state\n",
    "\n",
    "\n",
    "def is_terminal():\n",
    "     #env.is_terminal or something\n",
    "     pass\n",
    "\n",
    "\n",
    "def get_next_state(state, action, piece, config):\n",
    "    next_state = state.copy()\n",
    "    for row in range(config.rows-1, -1, -1):\n",
    "        if next_state[row][action] == 0:\n",
    "            break\n",
    "    next_state[row][action] = piece\n",
    "    return next_state\n",
    "\n",
    "\n",
    "def get_reward(state):\n",
    "    if is_terminal(state):\n",
    "        # if won\n",
    "            return 1\n",
    "        # else we lost:\n",
    "            return -1\n",
    "    return 0\n",
    "\n",
    "class QTable:\n",
    "    def __init__(self, n_action_space):\n",
    "        self.table = dict()\n",
    "        self.action_space = n_action_space\n",
    "        \n",
    "    def add_item(self, state_key):\n",
    "        self.table[state_key] = list(np.zeros(self.action_space))\n",
    "        \n",
    "    def __call__(self, state):\n",
    "        board = state.board[:] # Get a copy\n",
    "        board.append(state.mark)\n",
    "        state_key = np.array(board).astype(str)\n",
    "        state_key = hex(int(''.join(state_key), 3))[2:]\n",
    "        if state_key not in self.table.keys():\n",
    "            self.add_item(state_key)\n",
    "        \n",
    "        return self.table[state_key]\n",
    "        \n",
    "\n",
    "class QLearner():\n",
    "    def __init__(self, q_table, alpha= 0.1, gamma=0.5, epsilon = 0.1):\n",
    "        self.alpha = alpha # float  \n",
    "        self.gamma = gamma # float\n",
    "        self.q = q_table # func or dict table (initial quality table of every state but state space is biiiiig) dict of dict(float)\n",
    "        self.epsilon  = epsilon # float, policy parameter eps-greedy exploration\n",
    "\n",
    "    def choose_action(self, state, valid_moves, learning = False): # eps-greedy policy        \n",
    "        if learning and random.random() < self.epsilon:\n",
    "            action = random.choice(valid_moves) # exploration\n",
    "        else:\n",
    "            move_quality = sorted([(self.q(state, act), act) for act in valid_moves]) # ascending sort\n",
    "            action = move_quality[-1][1] # the best quality is last, the action is the column id on the idx = 1 element of move_quality\n",
    "        return action\n",
    "    \n",
    "    def update_quality(self, action, state):\n",
    "        # action is a move\n",
    "        # state is a tbale of connectx\n",
    "        if is_terminal(state):\n",
    "            self.q(state)[:] = 0\n",
    "        \n",
    "        else:\n",
    "            new_state = get_next_state(state, action)\n",
    "            self.q(state)[action] += + self.alph * (get_reward(new_state) + self.gamma * max(self.q(new_state)) - self.q(state)[action])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1\n",
    "gamma = 0.6\n",
    "epsilon = 0.99\n",
    "min_epsilon = 0.1\n",
    "\n",
    "episodes = 10000\n",
    "\n",
    "alpha_decay_step = 1000\n",
    "alpha_decay_rate = 0.9\n",
    "epsilon_decay_rate = 0.9999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all encompassing for loop\n",
    "q_table = QTable(env.configuration['columns'])\n",
    "q_learner = QLearner()\n",
    "\n",
    "games = 1000\n",
    "plays = 1000\n",
    "\n",
    "for g in range(games):\n",
    "\n",
    "    state = env.reset()\n",
    "    epochs, total_reward = 0, 0\n",
    "    done = False\n",
    "\n",
    "    for p in range(plays):\n",
    "\n",
    "\n",
    "        \n",
    "        # read state\n",
    "\n",
    "        # pick a move\n",
    "\n",
    "        # make a move\n",
    "\n",
    "        # observe the next state\n",
    "\n",
    "        # update the DynamicQTable\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_agent(obs, config):\n",
    "    # intiialise the learner\n",
    "    q_player = QLearner(q_init)\n",
    "\n",
    "    # Prepare state and valid moves\n",
    "    state = np.array(obs.board)\n",
    "    valid_moves = [col for col in range(config.columns) if state[col] == 0]\n",
    "    # state = preprocess_state(obs.board, obs.mark)\n",
    "\n",
    "    return q_player.choose_action(state, valid_moves, learning = False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a function to train your model\n",
    "\n",
    "def train_model():\n",
    "    # read the state (the board)\n",
    "\n",
    "    # decide on an action\n",
    "\n",
    "    # read the consequence of the action\n",
    "\n",
    "    # adapt the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save your model's weights/parameters in a local file (json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrtie a submission.py file with the model declaration\n",
    "# set the model parameters/weights using the saved json post training\n",
    "# define the agent's actions according to the model\n",
    "# et voile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_agent_to_file(filename=\"submission.py\", **params):\n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(\"\"\"\n",
    "#### MAKE FULL MODEL ####\n",
    "                \n",
    "#### IMPORT MODEL PARAMETERS FROM LOCAL JSON FILE####\n",
    "\n",
    "def my_agent(obs, config):\n",
    "    # Initialize and load the Actor model\n",
    "    actor = Actor(input_shape=43, output_shape=config.columns)\n",
    "    load_weights_into_model(actor, actor_weights)\n",
    "    actor.eval()\n",
    "\n",
    "    critic = Critic(input_shape=43)\n",
    "    load_weights_into_model(critic, critic_weights)\n",
    "    critic.eval()\n",
    "    \n",
    "    # Prepare state and valid moves\n",
    "    board = np.array(obs.board)\n",
    "    valid_moves = [col for col in range(config.columns) if board[col] == 0]\n",
    "    state = preprocess_state(obs.board, obs.mark)\n",
    "    \n",
    "    \n",
    "    action = get_action(state, valid_moves, board.flatten(), obs.mark, 3-obs.mark, config.rows, config.columns)\n",
    "    # Mask invalid moves\n",
    "    if action not in valid_moves:\n",
    "        action = random.choice(valid_moves)\n",
    "    return int(action)\n",
    "        \"\"\")\n",
    "    print(f\"Submission file {filename} created successfully!\")\n",
    "\n",
    "\n",
    "write_agent_to_file(filename=\"submission.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observe your model play against itself\n",
    "\n",
    "from kaggle_environments import make\n",
    "import sys\n",
    "\n",
    "# Import the agent directly\n",
    "from submission import my_agent\n",
    "\n",
    "# Run the environment with the agent\n",
    "env = make(\"connectx\", debug=True)\n",
    "env.run([my_agent, my_agent])\n",
    "\n",
    "print(\"Success!\" if env.state[0].status == env.state[1].status == \"DONE\" else \"Failed...\")\n",
    "\n",
    "# Display the game\n",
    "env.render(mode=\"ipython\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.random()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
